<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>CUDA</title>
      <link href="/2019/09/30/CUDA/"/>
      <url>/2019/09/30/CUDA/</url>
      
        <content type="html"><![CDATA[<p><excerpt in index | 首页摘要><br>记录CUDA编程学习过程中的问题。<br><a id="more"></a></excerpt></p><the rest of contents | 余下全文><h3 id="组织线程（https-www-cnblogs-com-1024incn-p-4537177-html）"><a href="#组织线程（https-www-cnblogs-com-1024incn-p-4537177-html）" class="headerlink" title="组织线程（https://www.cnblogs.com/1024incn/p/4537177.html）"></a>组织线程（<a href="https://www.cnblogs.com/1024incn/p/4537177.html）" target="_blank" rel="noopener">https://www.cnblogs.com/1024incn/p/4537177.html）</a></h3><p>掌握如何组织线程是CUDA编程的重要部分。CUDA线程分成Grid和Block两个层次。</p></the>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 记录 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Errors</title>
      <link href="/2019/09/07/Errors/"/>
      <url>/2019/09/07/Errors/</url>
      
        <content type="html"><![CDATA[<p><excerpt in index | 首页摘要><br>记录实践过程中遇到的问题。<br><a id="more"></a></excerpt></p><the rest of contents | 余下全文><h2 id="Caffe"><a href="#Caffe" class="headerlink" title="Caffe"></a>Caffe</h2><p>1.将mnist压缩包用工具解压后运行creat_mnist.sh，运行失败，提示无法打开数据：</p><ul><li>原因是工具解压后的数据caffe无法识别（自动创建了一个未知数据类型），参考data/get_mnist.sh后发现应该用gunzip {filename}.gz，该解压方式解压得到二进制文件。</li></ul><ol start="2"><li><code>ImportError: No module named cv2</code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m pip install opencv-python</span><br></pre></td></tr></table></figure></li></ol><h2 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h2><p>1.Ubuntu16.04切换python3和python2</p><ul><li>切换<code>python3</code>为默认版本<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo update-alternatives --install /usr/bin/python python /usr/bin/python2 100</span><br><span class="line">sudo update-alternatives --install /usr/bin/python python /usr/bin/python3 150</span><br></pre></td></tr></table></figure></li></ul><ul><li>切换<code>python2</code>为默认版本<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo update-alternatives --config python</span><br></pre></td></tr></table></figure></li></ul></the>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Errors </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OpenCV</title>
      <link href="/2019/08/28/OpenCV/"/>
      <url>/2019/08/28/OpenCV/</url>
      
        <content type="html"><![CDATA[<p><excerpt in index | 首页摘要><br>记录有关OpenCV的基础知识。<br><a id="more"></a></excerpt></p><the rest of contents | 余下全文><h3 id="提取Mat矩阵的感兴趣区域"><a href="#提取Mat矩阵的感兴趣区域" class="headerlink" title="提取Mat矩阵的感兴趣区域"></a>提取Mat矩阵的感兴趣区域</h3><ul><li>使用构造函数,如下例所示:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">//创建宽度为 320,高度为 240 的 3 通道图像</span><br><span class="line">Mat img(Size(320,240),CV_8UC3);</span><br><span class="line">//roi 是表示 img 中 Rect(10,10,100,100)区域的对象</span><br><span class="line">Mat roi(img, Rect(10,10,100,100));</span><br></pre></td></tr></table></figure><ul><li>使用括号运算符,如下:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Mat roi = img(Rect(10,10,100,100));</span><br></pre></td></tr></table></figure><ul><li>使用 Range 对象来定义感兴趣区域,如下:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">//使用括号运算符</span><br><span class="line">Mat roi = img(Range(10,100),Range(10,100));</span><br><span class="line">//使用构造函数</span><br><span class="line">Mat roi(img, Range(10,100),Range(10,100));</span><br></pre></td></tr></table></figure><h3 id="Mat矩阵的合并与图像的拼接"><a href="#Mat矩阵的合并与图像的拼接" class="headerlink" title="Mat矩阵的合并与图像的拼接"></a>Mat矩阵的合并与图像的拼接</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">- vconcat（B,C，A）; // 等同于A=[B ;C]</span><br><span class="line">- hconcat（B,C，A）; // 等同于A=[B  C]</span><br></pre></td></tr></table></figure><h3 id="复制Mat对象"><a href="#复制Mat对象" class="headerlink" title="复制Mat对象"></a>复制Mat对象</h3><h4 id="部分复制"><a href="#部分复制" class="headerlink" title="部分复制"></a>部分复制</h4><p>该类方法不会复制Mat对象的数据部分(即指针指向的矩阵)，只会复制它的头(包括矩阵的尺寸/存储方法/存储地址等信息)和指针部分(指向存储所有像素值的矩阵)。</p><ul><li>方法1：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Mat B(A);//B是A的部分复制</span><br></pre></td></tr></table></figure><ul><li>方法2(创建与Mat对象大小，类型相同的复制图像(数据可以自行设置))：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Mat B;</span><br><span class="line">B.create(A.size(), A.type());</span><br></pre></td></tr></table></figure><h4 id="完全复制-构造完全拷贝图像"><a href="#完全复制-构造完全拷贝图像" class="headerlink" title="完全复制(构造完全拷贝图像)"></a>完全复制(构造完全拷贝图像)</h4><p>注意：<br>(1) 不要直接对源图像操作，要先构造一个源图像的完全拷贝图像。<br>(2) 完全复制：把Mat对象的头部和数据部分一起复制。</p><ul><li>方法1：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Mat B = A.clone();</span><br></pre></td></tr></table></figure><ul><li>方法2：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Mat B;</span><br><span class="line">A.copyTo(B);</span><br></pre></td></tr></table></figure></the>]]></content>
      
      
      <categories>
          
          <category> 项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OpenCV基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++</title>
      <link href="/2019/08/23/C++/"/>
      <url>/2019/08/23/C++/</url>
      
        <content type="html"><![CDATA[<p><excerpt in index | 首页摘要><br>一些记录！<br><a id="more"></a></excerpt></p><the rest of contents | 余下全文><p>在学习C++的过程中我们经常会用到.和::和：和-&gt;，在此整理一下这些常用符号的区别。 </p><pre><code>1、A.B则A为对象或者结构体；2、A-&gt;B则A为指针，-&gt;是成员提取，A-&gt;B是提取A中的成员B，A只能是指向类、结构、联合的指针；3、::是作用域运算符，A::B表示作用域A中的名称B，A可以是名字空间、类、结构；4、：一般用来表示继承；</code></pre><p>缺省值（quē shěng zhí）就是默认值。是指一个属性、参数在被修改前的初始值。<br>计算机软件系统要求用户输入某些值而用户未给定时，系统自动赋予的事先设定的数值</p><h3 id="malloc"><a href="#malloc" class="headerlink" title="malloc"></a>malloc</h3><ul><li>malloc(sizeof(int)*num): num为需要保存的int类型的数，sizeof(int)为int类型数据占据的字节数</li></ul></the>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Linux</title>
      <link href="/2019/08/23/Linux/"/>
      <url>/2019/08/23/Linux/</url>
      
        <content type="html"><![CDATA[<p>PATH: 将命令所在的目录添加到PATH中，可以在当前用户目录下任何位置执行该命令<br>PATH=”$PATH”:/path</p><p>which: 查找命令文件位置，默认查找PATH内所规范的目录。which查不到cd命令是因为cd为bash内置命令。<br>whereis: 查询文件</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>YUV</title>
      <link href="/2019/08/23/YUV/"/>
      <url>/2019/08/23/YUV/</url>
      
        <content type="html"><![CDATA[<p><excerpt in index | 首页摘要><br>记录有关YUV的基础知识。<br><a id="more"></a></excerpt></p><the rest of contents | 余下全文><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>YUV是一种颜色编码方法。YUV分为三个分量，“Y”表示明亮度（Luminance或Luma），也就是灰度值；而“U”和“V” 表示的则是色度（Chrominance或Chroma），作用是描述影像色彩及饱和度，用于指定像素的颜色。现在的YUV是通常用于计算机领域用来表示使用YCbCr编码的文件。所以可以粗浅地视YUV为YCbCr。</p><h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><p>YUV主要用于电视系统以及模拟视频领域，YUV将亮度信息Y与色彩信息U/V分离，没有U/V信息一样可以显示完整的图像，只不过是黑白的，这样的设计很好地解决了彩色电视机与黑白电视的兼容问题。并且，YUV不像RGB那样要求三个独立的视频信号同时传输，所以用YUV方式传送占用极少的频宽。</p><h3 id="采样方式"><a href="#采样方式" class="headerlink" title="采样方式"></a>采样方式</h3><p>YUV码流的存储格式其实与其采样的方式密切相关，主流的采样方式有三种，YUV4:4:4，YUV4:2:2，YUV4:2:0</p><ul><li>YUV 4:4:4采样，每一个Y对应一组UV分量。经过8比特量化之后，每个像素(Y,U,V三个通道，每个通道8bit,占1个字节)占用3个字节</li><li>YUV 4:2:2采样，每两个Y共用一组UV分量。 </li><li>YUV 4:2:0采样，每四个Y共用一组UV分量。</li></ul><p><table><tr><td><center><img src="https://static.oschina.net/uploads/img/201412/08141454_ACE9.jpg" alt="YUV采样示意图"><br>YUV采样示意图</center></td></tr></table></p><h3 id="存储格式"><a href="#存储格式" class="headerlink" title="存储格式"></a>存储格式</h3><p>两大类：planar和packed。<br>YUV formats fall into two distinct groups, the packed formats where Y, U (Cb) and V (Cr) samples are packed together into macropixels which are stored in a single array, and the planar formats where each component is stored as a separate array, the final image being a fusing of the three separate planes.(plane not channel!)</p><ul><li>对于planar的YUV格式，使用三个数组分开存放YUV三个分量，每个数组分别存储所有像素点的Y、U、V分量。先连续存储所有像素点的Y，紧接着存储所有像素点的U，随后是所有像素点的V。</li><li>对于packed的YUV格式，YUV分量存放在同一个数组中，通常是几个相邻的像素组成一个宏像素(macro-pixel)，每个像素点的Y,U,V是连续交叉存储的。</li></ul><h2 id="YUV420"><a href="#YUV420" class="headerlink" title="YUV420"></a>YUV420</h2><p>YUV420格式是指，每个像素都保留一个Y（亮度）分量，而在水平方向上，不是每行都取U和V分量，而是一行只取U分量，则其接着一行就只取V分量，以此重复(即4:2:0, 4:0:2, 4:2:0, 4:0:2 …….)，所以420不是指没有V，而是指一行采样只取U，另一行采样只取V。在取U和V时，每两个Y之间取一个U或V。但从4x4矩阵列来看，每4个矩阵点Y区域中，只有一个U和V，所以它们的比值是4:1。所以对于一个像素，RGB需要8 * 3 = 24位，即占3个字节；而YUV420P，8 + 8/4 + 8/4 = 12位，即占2个字节，其中8指Y分量，8/4指U和V分量。</p><h3 id="存储方式"><a href="#存储方式" class="headerlink" title="存储方式"></a>存储方式</h3><p>在YUV420中，一个像素点对应一个Y，一个4X4的小方块对应一个U和V。对于所有YUV420图像，它们的Y值排列是完全相同的。YUV420主要有YUV420p与YUV420sp两种数据格式。YUV420sp与YUV420p的数据格式的UV排列在原理上是完全不同的。</p><ul><li>YUV420p：Y,U,V三个分量都是平面格式，根据U和V的相对位置可以分为I420和YV12，在I420格式U在V前面（即：YUV），但YV12则是相反（即：YVU）。YUV420p格式数据先把U存放完后，再存放V，也就是说UV它们是连续的。</li><li>YUV420sp：Y分量平面格式，UV打包格式，根据U和V的相对位置可以分为NV12和NV21。YUV420sp格式数据是UVUV交替存放的。</li></ul><p><strong>YV12/I420/YU12/NV12/NV21 都属于 YUV 4:2:0。</strong></p><ul><li>YU12就是I420，在 I420(YU12) 格式中，U 平面紧跟在 Y 平面之后，然后才是 V 平面（即：YUV）；但 YV12 则是相反（即：YVU），YV12 取名来源是 Y 后面紧跟 V（然后是 U），12 表示它位深为 12，也就是一个像素占用空间为 12 位。YV12和I420 也称为 YUV420p（即平面格式，planar），YV12 与标准模式 I420 的区别只是 UV 顺序不同。</li><li>大部分视频解码器的输出的原始图像都是 YUV420 格式（例如安卓下的图像通常都是 I420 或 NV21），而多数硬解码器中使用的都是 NV12 格式（例如 Intel MSDK、NVIDIA 的 cuvid、IOS 硬解码）。NV12属于另一类 YUV420sp, Y 分量平面格式，UV 打包格式。 NV12 与 NV21 类似，U 和 V 交错排列，不同在于 UV 顺序。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">I420: YYYYYYYY UU VV    =&gt;YUV420P</span><br><span class="line">YV12: YYYYYYYY VV UU    =&gt;YUV420P</span><br><span class="line">NV12: YYYYYYYY UVUV     =&gt;YUV420SP</span><br><span class="line">NV21: YYYYYYYY VUVU     =&gt;YUV420SP</span><br></pre></td></tr></table></figure><p><table><tr><td><center><img src="http://notes.maxwi.com/2017/12/05/yuv/yuv4.png" alt="YUV420p"></center></td></tr></table><br>可以看出 Y1、Y2、Y7、Y8 共用 U1 和 V1。后面的线性数组为其存储顺序，可以看出 Y、U 和 V 都是顺序存储的，往外写的时候，先按顺序将 Y 分量写出，然后再根据 U、V 分别将它们依次写出即可。</p><p><table><tr><td><center><img src="http://notes.maxwi.com/2017/12/05/yuv/yuv5.png" alt="YUV420sp"></center></td></tr></table><br>可以看出与 YV12 不同的时，它的 Y 虽然也是顺序存储，但 U、V 却是交错存储的，这种方式存储在往外写出时则先直接顺序写出 Y，然后根据 UV 打包格式将UV数据写出。</p></the>]]></content>
      
      
      <categories>
          
          <category> 项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> YUV基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python</title>
      <link href="/2019/08/21/Python/"/>
      <url>/2019/08/21/Python/</url>
      
        <content type="html"><![CDATA[<p><excerpt in index | 首页摘要><br>记录Python实践过程中遇到的问题。<br><a id="more"></a></excerpt></p><the rest of contents | 余下全文></the>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Skill </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Caffe</title>
      <link href="/2019/08/21/Caffe/"/>
      <url>/2019/08/21/Caffe/</url>
      
        <content type="html"><![CDATA[<p><excerpt in index | 首页摘要><br>记录Caffe实践过程中遇到的问题。<br><a id="more"></a></excerpt></p><the rest of contents | 余下全文><p><a href="http://caffe.berkeleyvision.org/" target="_blank" rel="noopener">Caffe</a>主要包括七大模块：</p><ul><li>Nets &lt;- Layers &lt;- Blobs: Caffe模型的骨架</li><li>Forward / Backward: 模型前向和后向计算</li><li>Loss: 损失函数值，模型优化策略即为损失函数最小</li><li>Solver: 规范协调模型优化过程</li><li>Layer Catalogue: 层是构建模型和计算模型的基础单元</li><li>Interfaces: 命令行 Python Matlab</li><li>Data: 数据caffe化，使其能够输入caffe模型</li></ul><p>Community models made by Caffe users are posted to a publicly editable <a href="https://github.com/BVLC/caffe/wiki/Model-Zoo" target="_blank" rel="noopener">model zoo wiki page</a></p><h3 id="Caffe模块"><a href="#Caffe模块" class="headerlink" title="Caffe模块"></a>Caffe模块</h3><h4 id="一、Nets-lt-Layers-lt-Blobs"><a href="#一、Nets-lt-Layers-lt-Blobs" class="headerlink" title="一、Nets &lt;- Layers &lt;- Blobs"></a>一、Nets &lt;- Layers &lt;- Blobs</h4><ul><li>Blobs<br>Caffe stores, communicates, and manipulates the information as blobs: the blob is the standard array and unified memory interface for the framework. The layer comes next as the foundation of both model and computation. The net follows as the collection and connection of layers. </li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">const Dtype* cpu_data() const; //不可改变</span><br><span class="line">Dtype* mutable_cpu_data();     //可以改变</span><br><span class="line"></span><br><span class="line">// Assuming that data are on the CPU initially, and we have a blob.</span><br><span class="line">const Dtype* foo;</span><br><span class="line">Dtype* bar;</span><br><span class="line">foo = blob.gpu_data(); // data copied cpu-&gt;gpu.</span><br><span class="line">foo = blob.cpu_data(); // no data copied since both cpu and gpu have up-to-date contents.</span><br><span class="line">bar = blob.mutable_gpu_data(); // 此操作前CPU数据不可变，因此cpu-&gt;gpu no data copied.</span><br><span class="line">blob.mutable_gpu_data();</span><br><span class="line"></span><br><span class="line">// Assuming that data are on the CPU initially, and we have a blob.</span><br><span class="line">const Dtype* foo;</span><br><span class="line">Dtype* bar;</span><br><span class="line">bar = blob.mutable_gpu_data(); // 此时数据只在cpu上，因此data copied cpu-&gt;gpu.</span><br><span class="line">bar = blob.mutable_cpu_data(); // 此操作前GPU上的数据可变，执行此操作会将GPU数据拷贝到CPU</span><br><span class="line">bar = blob.mutable_gpu_data(); // 此操作前CPU上的数据可变，执行此操作会将CPU数据拷贝到GPU</span><br></pre></td></tr></table></figure><ul><li>Layers</li></ul><p>A layer takes input through bottom connections and makes output through top connections.</p><p>Each layer type defines three critical computations: setup, forward, and backward.</p><p>Setup: initialize the layer and its connections once at model initialization.<br>Forward: given input from bottom compute the output and send to the top.<br>Backward: given the gradient w.r.t. the top output compute the gradient w.r.t. to the input and send to the bottom. A layer with parameters computes the gradient w.r.t. to its parameters and stores it internally.</p><ul><li>Nets</li></ul><p>The net jointly defines a function and its gradient by composition and auto-differentiation. The composition of every layer’s output computes the function to do a given task, and the composition of every layer’s backward computes the gradient from the loss to learn the task. Caffe models are end-to-end machine learning engines.</p><ul><li>Model Format</li></ul><p>The models are defined in plaintext protocol buffer schema (prototxt) while the learned models are serialized as binary protocol buffer (binaryproto) .caffemodel files.</p><h4 id="二、Forward-Backward"><a href="#二、Forward-Backward" class="headerlink" title="二、Forward/Backward"></a>二、Forward/Backward</h4><p>The forward pass computes the output given the input for inference. The backward pass computes the gradient given the loss for learning. In backward Caffe reverse-composes the gradient of each layer to compute the gradient of the whole model by automatic differentiation. </p><table><tr><td><center><img src="http://caffe.berkeleyvision.org/tutorial/fig/backward.jpg" alt="Forward/Backward" height="300"><br>Forward/Backward</center></td></tr></table><p>These computations follow immediately from defining the model: Caffe plans and carries out the forward and backward passes for you.</p><p>The Net::Forward() and Net::Backward() methods carry out the respective passes while Layer::Forward() and Layer::Backward() compute each step.<br>Every layer type has forward_{cpu,gpu}() and backward_{cpu,gpu}() methods to compute its steps according to the mode of computation. A layer may only implement CPU or GPU mode due to constraints or convenience.</p><h4 id="三、Loss"><a href="#三、Loss" class="headerlink" title="三、Loss"></a>三、Loss</h4><p>In Caffe, as in most of machine learning, learning is driven by a loss function (also known as an error, cost, or objective function). The goal of learning is to find a setting of the weights that minimizes the loss function.<br>The loss in Caffe is computed by the Forward pass of the network.<br>By convention, Caffe layer types with the suffix Loss contribute to the loss function, but other layers are assumed to be purely used for intermediate computations. However, any layer can be used as a loss by adding a field loss_weight: <float> to a layer definition for each top blob produced by the layer. Layers with the suffix Loss have an implicit loss_weight: 1 for the first top blob (and loss_weight: 0 for any additional tops); other layers have an implicit loss_weight: 0 for all tops.<br>The final loss in Caffe, then, is computed by summing the total weighted loss over the network:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">loss := 0</span><br><span class="line">for layer in layers:</span><br><span class="line">  for top, loss_weight in layer.tops, layer.loss_weights:</span><br><span class="line">    loss += loss_weight * sum(top)</span><br></pre></td></tr></table></figure></float></p><h4 id="四、Solver"><a href="#四、Solver" class="headerlink" title="四、Solver"></a>四、Solver</h4><p>The solver orchestrates model optimization by coordinating the network’s forward inference and backward gradients to form parameter updates that attempt to improve the loss. The responsibilities of learning are divided between the Solver for overseeing the optimization and generating parameter updates and the Net for yielding loss and gradients.</p><ul><li>The Caffe solvers are:</li></ul><p>Stochastic Gradient Descent (type: “SGD”),<br>AdaDelta (type: “AdaDelta”),<br>Adaptive Gradient (type: “AdaGrad”),<br>Adam (type: “Adam”),<br>Nesterov’s Accelerated Gradient (type: “Nesterov”) and<br>RMSprop (type: “RMSProp”)</p><p>The solver</p><p>1.scaffolds the optimization bookkeeping and creates the training network for learning and test network(s) for evaluation.<br>2.iteratively optimizes by calling forward / backward and updating parameters<br>3.(periodically) evaluates the test networks<br>4.snapshots the model and solver state throughout the optimization</p><p>where each iteration</p><p>1.calls network forward to compute the output and loss<br>2.calls network backward to compute the gradients<br>3.incorporates the gradients into parameter updates according to the solver method<br>4.updates the solver state according to learning rate, history, and method</p><p>to take the weights all the way from initialization to learned model.</p><p>Like Caffe models, Caffe solvers run in CPU / GPU modes.</p><ul><li>Solver Scaffolding<br>The solver scaffolding prepares the optimization method and initializes the model to be learned in Solver::Presolve().<br>Once we begin to train our model, it will initialize solver, net and loss(backward pass) with parameters from <strong>solver definition prototxt</strong>.<br>In the <strong>solver definition prototxt</strong>, solver states and weights can be configured to save in a specific directory. Weights are saved with .caffemodel extension while solver states are saved with .solverstate extension. .solverstate include more info than .caffemodel, like net.blobs, learning rate and so on at that time.</li></ul><h4 id="五、Layer-Catalogue"><a href="#五、Layer-Catalogue" class="headerlink" title="五、Layer Catalogue"></a>五、Layer Catalogue</h4><h4 id="六、Interfaces"><a href="#六、Interfaces" class="headerlink" title="六、Interfaces"></a>六、Interfaces</h4><ul><li><a href="https://github.com/BVLC/caffe/blob/master/docs/multigpu.md" target="_blank" rel="noopener">Multi-gpu</a></li></ul><p>each GPU runs the batchsize specified in your train_val.prototxt. So if you go from 1 GPU to 2 GPU, your effective batchsize will double. e.g. if your train_val.prototxt specified a batchsize of 256, if you run 2 GPUs your effective batch size is now 512. So you need to adjust the batchsize when running multiple GPUs and/or adjust your solver params, specifically learning rate.</p><p>Current implementation has a “soft” assumption that the devices being used are homogeneous. In practice, any devices of the same general class should work together, but performance and total size is limited by the smallest device being used. e.g. if you combine a TitanX and a GTX980, performance will be limited by the 980. Mixing vastly different levels of boards, e.g. Kepler and Fermi, is not supported.</p><p>Performance is heavily dependent on the PCIe topology of the system, the configuration of the neural network you are training, and the speed of each of the layers. </p><h4 id="七、Data"><a href="#七、Data" class="headerlink" title="七、Data"></a>七、Data</h4><h3 id="Caffe工具"><a href="#Caffe工具" class="headerlink" title="Caffe工具"></a>Caffe工具</h3><h4 id="feature-extract"><a href="#feature-extract" class="headerlink" title="feature extract"></a><a href="http://caffe.berkeleyvision.org/gathered/examples/feature_extraction.html" target="_blank" rel="noopener">feature extract</a></h4></the>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Caffe </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>云 念</title>
      <link href="/2019/03/22/%E4%BA%91%E5%BF%B5/"/>
      <url>/2019/03/22/%E4%BA%91%E5%BF%B5/</url>
      
        <content type="html"><![CDATA[<p><excerpt in index | 首页摘要><br>一些感悟！<br><a id="more"></a></excerpt></p><the rest of contents | 余下全文><p>2019-3-22<br>  回来的车上，我一直在想，如果当年追着那个小女孩的老头真的是人贩子的话，我想我会后悔一生。“应该不是吧，这么久也没有听说过有人贩子在大学城出没。”我就这样麻醉自己，利用自己的闭塞的消息通道来劝慰自己。我知道这句话经不起推敲，毕竟真正的黑暗是摆在我们普通人的眼前，我们依然会被它的深邃所迷惑。我只有祈祷，祈祷这个世界变得更加美好；我只有祈祷，祈祷下次再遇见类似事件时，自己能够有勇气摒弃路人冷眼旁观时的说辞，鼓起勇气弄清真相。</p><p>2020-2-20<br>  高三下半段，班主任安排考上清华北大的上届师兄师姐为我们答疑解惑。见面会最后的答疑环节，我忍不住举手站起身问到：当年你们天天这么读书，难道不怕受应试教育影响导致思维僵化吗？</p></the>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 感悟 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deep Learning</title>
      <link href="/2019/03/18/Deep-Learning/"/>
      <url>/2019/03/18/Deep-Learning/</url>
      
        <content type="html"><![CDATA[<p><excerpt in index | 首页摘要><br>阅读过程中的摘记。<br><a id="more"></a></excerpt></p><the rest of contents | 余下全文><h2 id="数学"><a href="#数学" class="headerlink" title="数学"></a>数学</h2><ol><li>线性代数</li></ol><ul><li>Hadamard乘积（元素对应相乘）：C=${A}\bigodot{B}$</li></ul><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><ol><li>深度学习</li></ol><ul><li>深度学习模型往往善于与训练数据拟合，但真正的挑战是泛化，而非拟合。</li></ul><ol start="2"><li>mAP与accuracy</li></ol><ul><li>多标签图像分类（Multi-label Image Classification）任务中图片的标签不止一个，因此评价不能用普通单标签图像分类的标准，即mean accuracy，该任务采用的是和信息检索中类似的方法—mAP（mean Average Precision）</li></ul><ol start="3"><li>Zero-Shot Learning</li></ol><ul><li>The underlying secret ensuring the success of zero-shot learning is to find an intermediate semantic representation (e.g. attributes or textual features) to transfer the knowledge learned from seen classes to unseen ones.(M. Elhoseiny, Y. Zhu, H. Zhang, and A. Elgammal. Link the head to the ”beak”: Zero shot learning from noisy text description at part precision. In CVPR, July 2017.)</li><li>provide the intra-class diversity while keeping inter-class discrimination for unseen novel classes. </li></ul><ol start="4"><li>边缘计算</li></ol><ul><li>在终端采集数据，并在终端完成数据处理，提供智能终端推断。</li><li>能够满足实时性、安全性的要求，且能节约带宽和存储空间。</li><li>智能将下沉到终端设备，智能边缘计算将会崛起。</li><li>在计算机视觉和语义识别等应用只中，终端采集数据，然后上传到云端处理的云计算对网络带宽和数据中心存储都带来了越来越大的挑战</li><li>无人驾驶等应用对实时性和安全性要求极高，网络的时延与稳定性带来的安全隐患是无人驾驶等应用无法忍受的。</li></ul><h2 id="周边"><a href="#周边" class="headerlink" title="周边"></a>周边</h2><ol><li>AI芯片</li></ol><ul><li>CPU：大部分面积被控制单元和缓存单元占据，只有少量的计算单元。指令执行过程包括取指令、指令译码和指令执行三部分。只有在指令执行时，计算单元才能发挥作用。串行计算</li><li>GPU：并行计算。指令执行过程如CPU一样包括三部分，因此制约了GPU计算能力</li><li>FPGA：现场可编程逻辑门阵列。算法即电路的芯片，接近I/O的高性能、低功耗芯片。硬件电路由算法定制，不需要取指令和指令译码的过程。可以重复编程</li><li>ASIC：专用集成电路。算法即电路，不需要取指令和指令译码的过程。为特定的需求而专门定制的芯片，能够最大程度的发挥芯片的计算能力。ASIC设计制造一旦完成，就无法进行改变。</li><li>四者能效：ASIC&gt;FPGA&gt;GPU&gt;CPU，通用性则相反</li><li>在云端模型训练中，GPU占主导地位，多GPU并行架构是云端训练常用的基础架构方案。</li><li>在云端识别中，基于功耗与运算速度的考量，单独基于GPU的方式并非最优方案，利用CPU、GPU、FPGA、ASIC各自的优点采用异构计算（CPU+GPU+FPGA/ASIC）是目前主流的方案。</li></ul><ol start="2"><li>工业相机<a href="http://www.51camera.com.cn/uploadfile//2018/0207/20180207015631933.pdf" target="_blank" rel="noopener">（视觉工程师必须知道的工业相机50问）</a></li></ol><ul><li>丢帧：该问题的出现与传输接口无关。设计不完善的驱动程序或工业相机硬件才是丢帧的真正原因。<br>（1）丢帧的实质：无法及时处理数据通道的堵塞问题。新的图像进来时，前一张可能被迫丢弃，或是新的图像被迫丢弃。<br>（2）问题解决：针对驱动程序与工业相机的数据传输等硬件上的每个环节进行精密设计。</li><li>CCD与CMOS相机<br>（1）二者的图像传感器光电转换原理相同，主要的差别在于信号的读出过程不同：CCD仅有一个或少数几个输出节点统一读出，信号的输出一致性非常好；CMOS每个像素都有各自的信号放大器，各自进行电荷-电压的转换，信号输出一致性较差。<br>（2）CMOS比CCD芯片的功耗要低，但是放大器的不一致性却带来了更高的固定噪声。<br>（3）CMOS芯片的集成度更高，芯片级相机。</li><li>分辨率<br>（1）芯片靶面排列的像元数量：H x V<br>（2）对同样大的视场（景物范围）成像时，分辨率越高，对细节的展示越明显</li><li>帧频和行频<br>（1）面阵相机用帧频表示，fps<br>（2）线阵相机用行频表示，KHz</li><li>噪声<br>（1）成像过程中不希望被采集到的、实际成像目标外的信号<br>（2）有效信号带来的符合泊松分布的统计涨落噪声<br>（3）相机自身固有的与信号无关的噪声</li><li>工业相机与普通数码相机<br>（1）工业相机快门时间特别短，能清晰地抓拍快速移动的物体，而普通相机抓拍快速移动的物体非常模糊<br>（2）工业相机的图像传感器是逐行扫描的，而普通相机的是隔行扫描<br>（3）拍摄速度<br>（4）工业相机输出的是裸数据，光谱范围也更宽，比较适合进行高质量的图像处理算法，普遍应用于机器视觉系统中；而普通相机的光谱范围只适合人眼视觉，并且经过了MPEG压缩，图像质量较差。</li><li>线阵相机<br>（1）线扫描传感器只有一行感光单元，每次只采集一行图像，每次只输出一行图像<br>（2）线阵相机的采集速度很快，用户可以选择每几行或者每十几行构成一帧图像进行处理，因此可以达到很高的帧率</li><li>CCD与CMOS芯片的主要参数<br>（1）像元尺寸反映了芯片对光的响应能力，像元尺寸越大，能够接收到的光子数量越多，在同样的光照条件和曝光时间内产生的电荷数量越多<br>（2）灵敏度：光器件的光电转换能力；器件所能传感的对地辐射功率<br>（3）坏点率<br>（4）光谱响应：芯片对于不同波长光线的响应能力，通常用光谱响应曲线给出</li></ul></the>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 记录 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
